{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "865e8764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import random as rn\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01c30d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82d102e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(35)\n",
    "rn.seed(35)\n",
    "tf.random.set_seed(35)\n",
    "#print(device_lib.list_local_devices()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc5946a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../../data/BG Ex data/\"\n",
    "data = np.load(\"../../data/BG_Fold_Info.npz\", allow_pickle=True)\n",
    "\n",
    "train_files = data[\"train\"]\n",
    "test_files = data[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96eefda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "param12 = {\n",
    "    \"FOLD_NUM\": [i for i in range(10)],\n",
    "    \"SEQ_LEN\": [12],\n",
    "    \"BATCH_SIZE\": [128, 256],\n",
    "    \"HIDDEN\": [12, 24, 36]\n",
    "}\n",
    "\n",
    "param24 = {\n",
    "    \"FOLD_NUM\": [i for i in range(10)],\n",
    "    \"SEQ_LEN\": [24],\n",
    "    \"BATCH_SIZE\": [128, 256],\n",
    "    \"HIDDEN\": [24, 48, 72]\n",
    "}\n",
    "\n",
    "param36 = {\n",
    "    \"FOLD_NUM\": [i for i in range(10)],\n",
    "    \"SEQ_LEN\": [36],\n",
    "    \"BATCH_SIZE\": [128, 256],\n",
    "    \"HIDDEN\": [36, 72, 108]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c392f3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_grid(dict_):\n",
    "    return pd.DataFrame([row for row in product(*dict_.values())], columns=dict_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92cd79aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FOLD_NUM</th>\n",
       "      <th>SEQ_LEN</th>\n",
       "      <th>BATCH_SIZE</th>\n",
       "      <th>HIDDEN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>128</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>128</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>128</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>256</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>256</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>9</td>\n",
       "      <td>36</td>\n",
       "      <td>128</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>9</td>\n",
       "      <td>36</td>\n",
       "      <td>128</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>9</td>\n",
       "      <td>36</td>\n",
       "      <td>256</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>9</td>\n",
       "      <td>36</td>\n",
       "      <td>256</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>9</td>\n",
       "      <td>36</td>\n",
       "      <td>256</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     FOLD_NUM  SEQ_LEN  BATCH_SIZE  HIDDEN\n",
       "0           0       12         128      12\n",
       "1           0       12         128      24\n",
       "2           0       12         128      36\n",
       "3           0       12         256      12\n",
       "4           0       12         256      24\n",
       "..        ...      ...         ...     ...\n",
       "175         9       36         128      72\n",
       "176         9       36         128     108\n",
       "177         9       36         256      36\n",
       "178         9       36         256      72\n",
       "179         9       36         256     108\n",
       "\n",
       "[180 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = pd.concat([expand_grid(param12), expand_grid(param24), expand_grid(param36)])\n",
    "params.reset_index(inplace=True)\n",
    "params.drop(\"index\", axis=1, inplace=True)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41525637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9d9699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_learning:\n",
    "    \n",
    "    def __init__(self, FOLD, SEQ, BATCH, HIDDEN):\n",
    "        \n",
    "        self.train_x = None\n",
    "        self.train_y = None\n",
    "        \n",
    "        self.test_x = None\n",
    "        self.test_y = None\n",
    "        \n",
    "        self.train_univ = None\n",
    "        self.train_univ = None\n",
    "        \n",
    "        self.test_univ = None\n",
    "        self.test_univ = None\n",
    "        \n",
    "        self.FOLD=FOLD\n",
    "        self.SEQ=SEQ\n",
    "        self.BATCH=BATCH\n",
    "        self.HIDDEN=HIDDEN\n",
    "        \n",
    "    def __del__(self):\n",
    "        \n",
    "        print(\"Remove Success\\n\")\n",
    "        \n",
    "    def Print_params(self):\n",
    "        \n",
    "        print(\"FOLD:\", self.FOLD)\n",
    "        print(\"SEQ:\", self.SEQ)\n",
    "        print(\"BATCH:\", self.BATCH)\n",
    "        print(\"HIDDEN:\", self.HIDDEN)\n",
    "        \n",
    "    def Add_data(self):\n",
    "        \n",
    "        train_data = []\n",
    "        train_x, train_y = [], []\n",
    "        \n",
    "        for file in train_files[self.FOLD]:\n",
    "            train_data.append(np.load(file_path+file+\".npz\"))\n",
    "            \n",
    "        for i in range(len(train_data)):\n",
    "            train_x.append(train_data[i][\"gen\"+str(self.SEQ)])\n",
    "            train_y.append(train_data[i][\"y\"+str(self.SEQ)].astype(np.float32))\n",
    "            \n",
    "        test_data = []\n",
    "        test_x, test_y = [], []\n",
    "        \n",
    "        for file in test_files[self.FOLD]:\n",
    "            test_data.append(np.load(file_path+file+\".npz\"))\n",
    "            \n",
    "        for i in range(len(test_data)):\n",
    "            test_x.append(test_data[i][\"gen\"+str(self.SEQ)])\n",
    "            test_y.append(test_data[i][\"y\"+str(self.SEQ)].astype(np.float32))\n",
    "            \n",
    "        self.train_x = np.concatenate(train_x).reshape(-1, self.SEQ, 1)\n",
    "        self.train_y = np.concatenate(train_y).reshape(-1, 1)\n",
    "        \n",
    "        self.test_x = np.concatenate(test_x).reshape(-1, self.SEQ, 1)\n",
    "        self.test_y = np.concatenate(test_y).reshape(-1, 1)\n",
    "            \n",
    "    def Print_data(self):\n",
    "        \n",
    "        print(\"Train:\", self.train_x.shape, self.train_y.shape)\n",
    "        print(\"Test:\", self.test_x.shape, self.test_y.shape)\n",
    "        \n",
    "    def Stack_data(self):\n",
    "\n",
    "        train_univ = tf.data.Dataset.from_tensor_slices((self.train_x, self.train_y))\n",
    "        self.train_univ = train_univ.cache().shuffle(len(self.train_x)).batch(self.BATCH).repeat()\n",
    "\n",
    "        test_univ = tf.data.Dataset.from_tensor_slices((self.test_x, self.test_y))\n",
    "        self.test_univ = test_univ.batch(self.BATCH).repeat()\n",
    "        \n",
    "    def RNN_learning(self):\n",
    "        \n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        save_model_path = \"../../model/Fold\"+str(self.FOLD)+\"/RNN_\"+str(self.SEQ)+\"_\"+str(self.HIDDEN)+\"_\"+str(self.BATCH)+\".h5\"\n",
    "        \n",
    "        if not os.path.exists(save_model_path):\n",
    "            with tf.device('/GPU:0'):\n",
    "                rnn_model = tf.keras.models.Sequential([\n",
    "                    tf.keras.layers.SimpleRNN(self.HIDDEN, input_shape=self.train_x.shape[-2:]),\n",
    "                    tf.keras.layers.Dense(1)\n",
    "                ])\n",
    "\n",
    "                rnn_model.compile(optimizer=\"adam\", loss=root_mean_squared_error)\n",
    "        \n",
    "        \n",
    "                rnn_model.fit(self.train_univ,\n",
    "                              epochs=100,\n",
    "                              steps_per_epoch=int(len(self.train_x)/self.BATCH),\n",
    "                              validation_data=self.test_univ,\n",
    "                              validation_steps=int(len(self.test_x)/self.BATCH))\n",
    "        \n",
    "            print(\"RNN learn complete\")\n",
    "        \n",
    "            rnn_model.save(save_model_path)\n",
    "\n",
    "    def LSTM_learning(self):\n",
    "        \n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        save_model_path = \"../../model/Fold\"+str(self.FOLD)+\"/LSTM_\"+str(self.SEQ)+\"_\"+str(self.HIDDEN)+\"_\"+str(self.BATCH)+\".h5\"\n",
    "        \n",
    "        if not os.path.exists(save_model_path):\n",
    "            with tf.device(\"/GPU:0\"):\n",
    "                lstm_model = tf.keras.models.Sequential([\n",
    "                    tf.keras.layers.LSTM(self.HIDDEN, input_shape=self.train_x.shape[-2:]),\n",
    "                    tf.keras.layers.Dense(1)\n",
    "                ])\n",
    "\n",
    "                lstm_model.compile(optimizer=\"adam\", loss=root_mean_squared_error)\n",
    "\n",
    "                lstm_model.fit(self.train_univ,\n",
    "                               epochs=100,\n",
    "                               steps_per_epoch=int(len(self.train_x)/self.BATCH),\n",
    "                               validation_data=self.test_univ,\n",
    "                               validation_steps=int(len(self.test_x)/self.BATCH))\n",
    "        \n",
    "            print(\"LSTM learn complete\")\n",
    "        \n",
    "            lstm_model.save(save_model_path)     \n",
    "    \n",
    "    def STACKLSTM_learning(self):\n",
    "        \n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        save_model_path = \"../../model/Fold\"+str(self.FOLD)+\"/STACKLSTM_\"+str(self.SEQ)+\"_\"+str(self.HIDDEN)+\"_\"+str(self.BATCH)+\".h5\"\n",
    "\n",
    "        if not os.path.exists(save_model_path):\n",
    "            with tf.device(\"/GPU:0\"):\n",
    "                stack_lstm_model = tf.keras.models.Sequential([\n",
    "                    tf.keras.layers.LSTM(self.HIDDEN, input_shape=self.train_x.shape[-2:], return_sequences=True),\n",
    "                    tf.keras.layers.LSTM(self.HIDDEN, return_sequences=False),\n",
    "                    tf.keras.layers.Dense(1)\n",
    "                ])\n",
    "\n",
    "                stack_lstm_model.compile(optimizer=\"adam\", loss=root_mean_squared_error)\n",
    "\n",
    "                stack_lstm_model.fit(self.train_univ,\n",
    "                                     epochs=100,\n",
    "                                     steps_per_epoch=int(len(self.train_x)/self.BATCH),\n",
    "                                     validation_data=self.test_univ,\n",
    "                                     validation_steps=int(len(self.test_x)/self.BATCH))\n",
    "        \n",
    "            print(\"STACKLSTM learn complete\")\n",
    "        \n",
    "            stack_lstm_model.save(save_model_path)     \n",
    "\n",
    "    def BILSTM_learning(self):\n",
    "        \n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        save_model_path = \"../../model/Fold\"+str(self.FOLD)+\"/BILSTM_\"+str(self.SEQ)+\"_\"+str(self.HIDDEN)+\"_\"+str(self.BATCH)+\".h5\"\n",
    "\n",
    "        if not os.path.exists(save_model_path):\n",
    "            with tf.device(\"/GPU:0\"):\n",
    "                bilstm_model = tf.keras.models.Sequential([\n",
    "                    tf.keras.layers.Bidirectional(\n",
    "                        tf.keras.layers.LSTM(self.HIDDEN),\n",
    "                        input_shape=self.train_x.shape[-2:]\n",
    "                    ),\n",
    "                    tf.keras.layers.Dense(1)\n",
    "                ])\n",
    "\n",
    "                bilstm_model.compile(optimizer=\"adam\", loss=root_mean_squared_error)\n",
    "\n",
    "                bilstm_model.fit(self.train_univ,\n",
    "                                 epochs=100,\n",
    "                                 steps_per_epoch=int(len(self.train_x)/self.BATCH),\n",
    "                                 validation_data=self.test_univ,\n",
    "                                 validation_steps=int(len(self.test_x)/self.BATCH))\n",
    "        \n",
    "            print(\"BILSTM learn complete\")\n",
    "        \n",
    "            bilstm_model.save(save_model_path)     \n",
    "\n",
    "    def GRU_learning(self):\n",
    "        \n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        save_model_path = \"../../model/Fold\"+str(self.FOLD)+\"/GRU_\"+str(self.SEQ)+\"_\"+str(self.HIDDEN)+\"_\"+str(self.BATCH)+\".h5\"\n",
    "        \n",
    "        if not os.path.exists(save_model_path):\n",
    "            with tf.device(\"/GPU:0\"):\n",
    "                gru_model = tf.keras.models.Sequential([\n",
    "                    tf.keras.layers.GRU(self.HIDDEN, input_shape=self.train_x.shape[-2:]),\n",
    "                    tf.keras.layers.Dense(1)\n",
    "                ])\n",
    "\n",
    "                gru_model.compile(optimizer=\"adam\", loss=root_mean_squared_error)\n",
    "\n",
    "                gru_model.fit(self.train_univ,\n",
    "                              epochs=100,\n",
    "                              steps_per_epoch=int(len(self.train_x)/self.BATCH),\n",
    "                              validation_data=self.test_univ,\n",
    "                              validation_steps=int(len(self.test_x)/self.BATCH))\n",
    "        \n",
    "            print(\"GRU learn complete\")\n",
    "        \n",
    "            gru_model.save(save_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56e08510",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 0\n",
      "SEQ: 12\n",
      "BATCH: 128\n",
      "HIDDEN: 12\n",
      "Train: (136038, 12, 1) (136038, 1)\n",
      "Test: (15189, 12, 1) (15189, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 0\n",
      "SEQ: 12\n",
      "BATCH: 128\n",
      "HIDDEN: 24\n",
      "Train: (136038, 12, 1) (136038, 1)\n",
      "Test: (15189, 12, 1) (15189, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 0\n",
      "SEQ: 12\n",
      "BATCH: 128\n",
      "HIDDEN: 36\n",
      "Train: (136038, 12, 1) (136038, 1)\n",
      "Test: (15189, 12, 1) (15189, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 0\n",
      "SEQ: 12\n",
      "BATCH: 256\n",
      "HIDDEN: 12\n",
      "Train: (136038, 12, 1) (136038, 1)\n",
      "Test: (15189, 12, 1) (15189, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 0\n",
      "SEQ: 12\n",
      "BATCH: 256\n",
      "HIDDEN: 24\n",
      "Train: (136038, 12, 1) (136038, 1)\n",
      "Test: (15189, 12, 1) (15189, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 0\n",
      "SEQ: 12\n",
      "BATCH: 256\n",
      "HIDDEN: 36\n",
      "Train: (136038, 12, 1) (136038, 1)\n",
      "Test: (15189, 12, 1) (15189, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 1\n",
      "SEQ: 12\n",
      "BATCH: 128\n",
      "HIDDEN: 12\n",
      "Train: (138369, 12, 1) (138369, 1)\n",
      "Test: (12858, 12, 1) (12858, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 1\n",
      "SEQ: 12\n",
      "BATCH: 128\n",
      "HIDDEN: 24\n",
      "Train: (138369, 12, 1) (138369, 1)\n",
      "Test: (12858, 12, 1) (12858, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 1\n",
      "SEQ: 12\n",
      "BATCH: 128\n",
      "HIDDEN: 36\n",
      "Train: (138369, 12, 1) (138369, 1)\n",
      "Test: (12858, 12, 1) (12858, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 1\n",
      "SEQ: 12\n",
      "BATCH: 256\n",
      "HIDDEN: 12\n",
      "Train: (138369, 12, 1) (138369, 1)\n",
      "Test: (12858, 12, 1) (12858, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 1\n",
      "SEQ: 12\n",
      "BATCH: 256\n",
      "HIDDEN: 24\n",
      "Train: (138369, 12, 1) (138369, 1)\n",
      "Test: (12858, 12, 1) (12858, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 1\n",
      "SEQ: 12\n",
      "BATCH: 256\n",
      "HIDDEN: 36\n",
      "Train: (138369, 12, 1) (138369, 1)\n",
      "Test: (12858, 12, 1) (12858, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 2\n",
      "SEQ: 12\n",
      "BATCH: 128\n",
      "HIDDEN: 12\n",
      "Train: (135029, 12, 1) (135029, 1)\n",
      "Test: (16198, 12, 1) (16198, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 2\n",
      "SEQ: 12\n",
      "BATCH: 128\n",
      "HIDDEN: 24\n",
      "Train: (135029, 12, 1) (135029, 1)\n",
      "Test: (16198, 12, 1) (16198, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 2\n",
      "SEQ: 12\n",
      "BATCH: 128\n",
      "HIDDEN: 36\n",
      "Train: (135029, 12, 1) (135029, 1)\n",
      "Test: (16198, 12, 1) (16198, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 2\n",
      "SEQ: 12\n",
      "BATCH: 256\n",
      "HIDDEN: 12\n",
      "Train: (135029, 12, 1) (135029, 1)\n",
      "Test: (16198, 12, 1) (16198, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 2\n",
      "SEQ: 12\n",
      "BATCH: 256\n",
      "HIDDEN: 24\n",
      "Train: (135029, 12, 1) (135029, 1)\n",
      "Test: (16198, 12, 1) (16198, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 2\n",
      "SEQ: 12\n",
      "BATCH: 256\n",
      "HIDDEN: 36\n",
      "Train: (135029, 12, 1) (135029, 1)\n",
      "Test: (16198, 12, 1) (16198, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 3\n",
      "SEQ: 12\n",
      "BATCH: 128\n",
      "HIDDEN: 12\n",
      "Train: (137240, 12, 1) (137240, 1)\n",
      "Test: (13987, 12, 1) (13987, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 3\n",
      "SEQ: 12\n",
      "BATCH: 128\n",
      "HIDDEN: 24\n",
      "Train: (137240, 12, 1) (137240, 1)\n",
      "Test: (13987, 12, 1) (13987, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 3\n",
      "SEQ: 12\n",
      "BATCH: 128\n",
      "HIDDEN: 36\n",
      "Train: (137240, 12, 1) (137240, 1)\n",
      "Test: (13987, 12, 1) (13987, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 3\n",
      "SEQ: 12\n",
      "BATCH: 256\n",
      "HIDDEN: 12\n",
      "Train: (137240, 12, 1) (137240, 1)\n",
      "Test: (13987, 12, 1) (13987, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 3\n",
      "SEQ: 12\n",
      "BATCH: 256\n",
      "HIDDEN: 24\n",
      "Train: (137240, 12, 1) (137240, 1)\n",
      "Test: (13987, 12, 1) (13987, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 3\n",
      "SEQ: 12\n",
      "BATCH: 256\n",
      "HIDDEN: 36\n",
      "Train: (137240, 12, 1) (137240, 1)\n",
      "Test: (13987, 12, 1) (13987, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 4\n",
      "SEQ: 12\n",
      "BATCH: 128\n",
      "HIDDEN: 12\n",
      "Train: (137271, 12, 1) (137271, 1)\n",
      "Test: (13956, 12, 1) (13956, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 4\n",
      "SEQ: 12\n",
      "BATCH: 128\n",
      "HIDDEN: 24\n",
      "Train: (137271, 12, 1) (137271, 1)\n",
      "Test: (13956, 12, 1) (13956, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 4\n",
      "SEQ: 12\n",
      "BATCH: 128\n",
      "HIDDEN: 36\n",
      "Train: (137271, 12, 1) (137271, 1)\n",
      "Test: (13956, 12, 1) (13956, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 4\n",
      "SEQ: 12\n",
      "BATCH: 256\n",
      "HIDDEN: 12\n",
      "Train: (137271, 12, 1) (137271, 1)\n",
      "Test: (13956, 12, 1) (13956, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 4\n",
      "SEQ: 12\n",
      "BATCH: 256\n",
      "HIDDEN: 24\n",
      "Train: (137271, 12, 1) (137271, 1)\n",
      "Test: (13956, 12, 1) (13956, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 4\n",
      "SEQ: 12\n",
      "BATCH: 256\n",
      "HIDDEN: 36\n",
      "Train: (137271, 12, 1) (137271, 1)\n",
      "Test: (13956, 12, 1) (13956, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 5\n",
      "SEQ: 12\n",
      "BATCH: 128\n",
      "HIDDEN: 12\n",
      "Train: (135452, 12, 1) (135452, 1)\n",
      "Test: (15775, 12, 1) (15775, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 5\n",
      "SEQ: 12\n",
      "BATCH: 128\n",
      "HIDDEN: 24\n",
      "Train: (135452, 12, 1) (135452, 1)\n",
      "Test: (15775, 12, 1) (15775, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 5\n",
      "SEQ: 12\n",
      "BATCH: 128\n",
      "HIDDEN: 36\n",
      "Train: (135452, 12, 1) (135452, 1)\n",
      "Test: (15775, 12, 1) (15775, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 5\n",
      "SEQ: 12\n",
      "BATCH: 256\n",
      "HIDDEN: 12\n",
      "Train: (135452, 12, 1) (135452, 1)\n",
      "Test: (15775, 12, 1) (15775, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 5\n",
      "SEQ: 12\n",
      "BATCH: 256\n",
      "HIDDEN: 24\n",
      "Train: (135452, 12, 1) (135452, 1)\n",
      "Test: (15775, 12, 1) (15775, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 5\n",
      "SEQ: 12\n",
      "BATCH: 256\n",
      "HIDDEN: 36\n",
      "Train: (135452, 12, 1) (135452, 1)\n",
      "Test: (15775, 12, 1) (15775, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 6\n",
      "SEQ: 12\n",
      "BATCH: 128\n",
      "HIDDEN: 12\n",
      "Train: (136825, 12, 1) (136825, 1)\n",
      "Test: (14402, 12, 1) (14402, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 6\n",
      "SEQ: 12\n",
      "BATCH: 128\n",
      "HIDDEN: 24\n",
      "Train: (136825, 12, 1) (136825, 1)\n",
      "Test: (14402, 12, 1) (14402, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 6\n",
      "SEQ: 12\n",
      "BATCH: 128\n",
      "HIDDEN: 36\n",
      "Train: (136825, 12, 1) (136825, 1)\n",
      "Test: (14402, 12, 1) (14402, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 6\n",
      "SEQ: 12\n",
      "BATCH: 256\n",
      "HIDDEN: 12\n",
      "Train: (136825, 12, 1) (136825, 1)\n",
      "Test: (14402, 12, 1) (14402, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 6\n",
      "SEQ: 12\n",
      "BATCH: 256\n",
      "HIDDEN: 24\n",
      "Train: (136825, 12, 1) (136825, 1)\n",
      "Test: (14402, 12, 1) (14402, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 6\n",
      "SEQ: 12\n",
      "BATCH: 256\n",
      "HIDDEN: 36\n",
      "Train: (136825, 12, 1) (136825, 1)\n",
      "Test: (14402, 12, 1) (14402, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 7\n",
      "SEQ: 12\n",
      "BATCH: 128\n",
      "HIDDEN: 12\n",
      "Train: (134120, 12, 1) (134120, 1)\n",
      "Test: (17107, 12, 1) (17107, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 7\n",
      "SEQ: 12\n",
      "BATCH: 128\n",
      "HIDDEN: 24\n",
      "Train: (134120, 12, 1) (134120, 1)\n",
      "Test: (17107, 12, 1) (17107, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 7\n",
      "SEQ: 12\n",
      "BATCH: 128\n",
      "HIDDEN: 36\n",
      "Train: (134120, 12, 1) (134120, 1)\n",
      "Test: (17107, 12, 1) (17107, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 7\n",
      "SEQ: 12\n",
      "BATCH: 256\n",
      "HIDDEN: 12\n",
      "Train: (134120, 12, 1) (134120, 1)\n",
      "Test: (17107, 12, 1) (17107, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 7\n",
      "SEQ: 12\n",
      "BATCH: 256\n",
      "HIDDEN: 24\n",
      "Train: (134120, 12, 1) (134120, 1)\n",
      "Test: (17107, 12, 1) (17107, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 7\n",
      "SEQ: 12\n",
      "BATCH: 256\n",
      "HIDDEN: 36\n",
      "Train: (134120, 12, 1) (134120, 1)\n",
      "Test: (17107, 12, 1) (17107, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 8\n",
      "SEQ: 12\n",
      "BATCH: 128\n",
      "HIDDEN: 12\n",
      "Train: (135380, 12, 1) (135380, 1)\n",
      "Test: (15847, 12, 1) (15847, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 8\n",
      "SEQ: 12\n",
      "BATCH: 128\n",
      "HIDDEN: 24\n",
      "Train: (135380, 12, 1) (135380, 1)\n",
      "Test: (15847, 12, 1) (15847, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 8\n",
      "SEQ: 12\n",
      "BATCH: 128\n",
      "HIDDEN: 36\n",
      "Train: (135380, 12, 1) (135380, 1)\n",
      "Test: (15847, 12, 1) (15847, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 8\n",
      "SEQ: 12\n",
      "BATCH: 256\n",
      "HIDDEN: 12\n",
      "Train: (135380, 12, 1) (135380, 1)\n",
      "Test: (15847, 12, 1) (15847, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 8\n",
      "SEQ: 12\n",
      "BATCH: 256\n",
      "HIDDEN: 24\n",
      "Train: (135380, 12, 1) (135380, 1)\n",
      "Test: (15847, 12, 1) (15847, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 8\n",
      "SEQ: 12\n",
      "BATCH: 256\n",
      "HIDDEN: 36\n",
      "Train: (135380, 12, 1) (135380, 1)\n",
      "Test: (15847, 12, 1) (15847, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 9\n",
      "SEQ: 12\n",
      "BATCH: 128\n",
      "HIDDEN: 12\n",
      "Train: (135319, 12, 1) (135319, 1)\n",
      "Test: (15908, 12, 1) (15908, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 9\n",
      "SEQ: 12\n",
      "BATCH: 128\n",
      "HIDDEN: 24\n",
      "Train: (135319, 12, 1) (135319, 1)\n",
      "Test: (15908, 12, 1) (15908, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 9\n",
      "SEQ: 12\n",
      "BATCH: 128\n",
      "HIDDEN: 36\n",
      "Train: (135319, 12, 1) (135319, 1)\n",
      "Test: (15908, 12, 1) (15908, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 9\n",
      "SEQ: 12\n",
      "BATCH: 256\n",
      "HIDDEN: 12\n",
      "Train: (135319, 12, 1) (135319, 1)\n",
      "Test: (15908, 12, 1) (15908, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 9\n",
      "SEQ: 12\n",
      "BATCH: 256\n",
      "HIDDEN: 24\n",
      "Train: (135319, 12, 1) (135319, 1)\n",
      "Test: (15908, 12, 1) (15908, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 9\n",
      "SEQ: 12\n",
      "BATCH: 256\n",
      "HIDDEN: 36\n",
      "Train: (135319, 12, 1) (135319, 1)\n",
      "Test: (15908, 12, 1) (15908, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 0\n",
      "SEQ: 24\n",
      "BATCH: 128\n",
      "HIDDEN: 24\n",
      "Train: (136038, 24, 1) (136038, 1)\n",
      "Test: (15189, 24, 1) (15189, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 0\n",
      "SEQ: 24\n",
      "BATCH: 128\n",
      "HIDDEN: 48\n",
      "Train: (136038, 24, 1) (136038, 1)\n",
      "Test: (15189, 24, 1) (15189, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 0\n",
      "SEQ: 24\n",
      "BATCH: 128\n",
      "HIDDEN: 72\n",
      "Train: (136038, 24, 1) (136038, 1)\n",
      "Test: (15189, 24, 1) (15189, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 0\n",
      "SEQ: 24\n",
      "BATCH: 256\n",
      "HIDDEN: 24\n",
      "Train: (136038, 24, 1) (136038, 1)\n",
      "Test: (15189, 24, 1) (15189, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 0\n",
      "SEQ: 24\n",
      "BATCH: 256\n",
      "HIDDEN: 48\n",
      "Train: (136038, 24, 1) (136038, 1)\n",
      "Test: (15189, 24, 1) (15189, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 0\n",
      "SEQ: 24\n",
      "BATCH: 256\n",
      "HIDDEN: 72\n",
      "Train: (136038, 24, 1) (136038, 1)\n",
      "Test: (15189, 24, 1) (15189, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 1\n",
      "SEQ: 24\n",
      "BATCH: 128\n",
      "HIDDEN: 24\n",
      "Train: (138369, 24, 1) (138369, 1)\n",
      "Test: (12858, 24, 1) (12858, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 1\n",
      "SEQ: 24\n",
      "BATCH: 128\n",
      "HIDDEN: 48\n",
      "Train: (138369, 24, 1) (138369, 1)\n",
      "Test: (12858, 24, 1) (12858, 1)\n",
      "Remove Success\n",
      "\n",
      "FOLD: 1\n",
      "SEQ: 24\n",
      "BATCH: 128\n",
      "HIDDEN: 72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (138369, 24, 1) (138369, 1)\n",
      "Test: (12858, 24, 1) (12858, 1)\n",
      "Epoch 1/100\n",
      "1081/1081 [==============================] - 23s 13ms/step - loss: 152.9677 - val_loss: 95.1359\n",
      "Epoch 2/100\n",
      "1081/1081 [==============================] - 14s 13ms/step - loss: 94.3741 - val_loss: 60.0904\n",
      "Epoch 3/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 57.3141 - val_loss: 30.2692\n",
      "Epoch 4/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 35.9777 - val_loss: 22.9320\n",
      "Epoch 5/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 27.0218 - val_loss: 20.4103\n",
      "Epoch 6/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 23.6024 - val_loss: 19.7703\n",
      "Epoch 7/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 22.3277 - val_loss: 19.1921\n",
      "Epoch 8/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.8559 - val_loss: 20.1439\n",
      "Epoch 9/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.6067 - val_loss: 19.3103\n",
      "Epoch 10/100\n",
      "1081/1081 [==============================] - 18s 17ms/step - loss: 21.4718 - val_loss: 19.0114\n",
      "Epoch 11/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.3765 - val_loss: 18.8962\n",
      "Epoch 12/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.3235 - val_loss: 19.6124\n",
      "Epoch 13/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.2858 - val_loss: 19.3334\n",
      "Epoch 14/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.2707 - val_loss: 18.8386\n",
      "Epoch 15/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.2833 - val_loss: 19.0633\n",
      "Epoch 16/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.2249 - val_loss: 19.0604\n",
      "Epoch 17/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.2042 - val_loss: 18.8838\n",
      "Epoch 18/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 21.1906 - val_loss: 18.9750\n",
      "Epoch 19/100\n",
      "1081/1081 [==============================] - 14s 13ms/step - loss: 21.2234 - val_loss: 18.8321\n",
      "Epoch 20/100\n",
      "1081/1081 [==============================] - 15s 14ms/step - loss: 21.1941 - val_loss: 19.0493\n",
      "Epoch 21/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.1258 - val_loss: 18.8129\n",
      "Epoch 22/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.1217 - val_loss: 18.7816\n",
      "Epoch 23/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.1471 - val_loss: 18.8264\n",
      "Epoch 24/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.1121 - val_loss: 18.7887\n",
      "Epoch 25/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.1208 - val_loss: 18.7869\n",
      "Epoch 26/100\n",
      "1081/1081 [==============================] - 17s 15ms/step - loss: 21.0991 - val_loss: 19.2515\n",
      "Epoch 27/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.0389 - val_loss: 19.1329\n",
      "Epoch 28/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.0851 - val_loss: 18.7729\n",
      "Epoch 29/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 21.0376 - val_loss: 18.8593\n",
      "Epoch 30/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.0626 - val_loss: 18.8197\n",
      "Epoch 31/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.9947 - val_loss: 19.0536\n",
      "Epoch 32/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.9880 - val_loss: 18.7854\n",
      "Epoch 33/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.9441 - val_loss: 18.9683\n",
      "Epoch 34/100\n",
      "1081/1081 [==============================] - 17s 15ms/step - loss: 21.0350 - val_loss: 19.0282\n",
      "Epoch 35/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.9792 - val_loss: 18.8633\n",
      "Epoch 36/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.9359 - val_loss: 18.8650\n",
      "Epoch 37/100\n",
      "1081/1081 [==============================] - 17s 15ms/step - loss: 21.0045 - val_loss: 18.7943\n",
      "Epoch 38/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.9061 - val_loss: 19.1067\n",
      "Epoch 39/100\n",
      "1081/1081 [==============================] - 18s 16ms/step - loss: 20.9487 - val_loss: 18.9175\n",
      "Epoch 40/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.9079 - val_loss: 18.7954\n",
      "Epoch 41/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.8907 - val_loss: 18.8779\n",
      "Epoch 42/100\n",
      "1081/1081 [==============================] - 17s 15ms/step - loss: 20.9358 - val_loss: 18.7675\n",
      "Epoch 43/100\n",
      "1081/1081 [==============================] - 17s 15ms/step - loss: 20.8581 - val_loss: 18.9110\n",
      "Epoch 44/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.9270 - val_loss: 18.9082\n",
      "Epoch 45/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.8288 - val_loss: 19.2856\n",
      "Epoch 46/100\n",
      "1081/1081 [==============================] - 17s 15ms/step - loss: 20.8237 - val_loss: 19.0614\n",
      "Epoch 47/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.8899 - val_loss: 18.7618\n",
      "Epoch 48/100\n",
      "1081/1081 [==============================] - 17s 15ms/step - loss: 20.8080 - val_loss: 18.8764\n",
      "Epoch 49/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 20.8502 - val_loss: 19.0664\n",
      "Epoch 50/100\n",
      "1081/1081 [==============================] - 17s 15ms/step - loss: 20.8241 - val_loss: 18.7444\n",
      "Epoch 51/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 20.7969 - val_loss: 18.8293\n",
      "Epoch 52/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.7907 - val_loss: 18.8230\n",
      "Epoch 53/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.7694 - val_loss: 18.8371\n",
      "Epoch 54/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.7862 - val_loss: 18.8657\n",
      "Epoch 55/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.7250 - val_loss: 18.7412\n",
      "Epoch 56/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 20.8355 - val_loss: 18.8014\n",
      "Epoch 57/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.7190 - val_loss: 18.9213\n",
      "Epoch 58/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.7776 - val_loss: 19.0972\n",
      "Epoch 59/100\n",
      "1081/1081 [==============================] - 18s 16ms/step - loss: 20.7153 - val_loss: 18.7648\n",
      "Epoch 60/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.6755 - val_loss: 18.7921\n",
      "Epoch 61/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 20.7326 - val_loss: 18.7978\n",
      "Epoch 62/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.7498 - val_loss: 18.8227\n",
      "Epoch 63/100\n",
      "1081/1081 [==============================] - 18s 17ms/step - loss: 20.6779 - val_loss: 19.2872\n",
      "Epoch 64/100\n",
      "1081/1081 [==============================] - 17s 15ms/step - loss: 20.6779 - val_loss: 18.8840\n",
      "Epoch 65/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.7222 - val_loss: 19.5404\n",
      "Epoch 66/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.6214 - val_loss: 18.7669\n",
      "Epoch 67/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.7122 - val_loss: 18.8195\n",
      "Epoch 68/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.6637 - val_loss: 18.8283\n",
      "Epoch 69/100\n",
      "1081/1081 [==============================] - 18s 17ms/step - loss: 20.6311 - val_loss: 18.9501\n",
      "Epoch 70/100\n",
      "1081/1081 [==============================] - 18s 16ms/step - loss: 20.5967 - val_loss: 18.7750\n",
      "Epoch 71/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.6200 - val_loss: 18.9493\n",
      "Epoch 72/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.6268 - val_loss: 18.9966\n",
      "Epoch 73/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 20.5732 - val_loss: 18.8992\n",
      "Epoch 74/100\n",
      "1081/1081 [==============================] - 15s 14ms/step - loss: 20.6397 - val_loss: 18.7882\n",
      "Epoch 75/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.6193 - val_loss: 18.8121\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1081/1081 [==============================] - 17s 15ms/step - loss: 20.5898 - val_loss: 19.1744\n",
      "Epoch 77/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.6116 - val_loss: 18.7957\n",
      "Epoch 78/100\n",
      "1081/1081 [==============================] - 18s 16ms/step - loss: 20.4417 - val_loss: 18.7532\n",
      "Epoch 79/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.5607 - val_loss: 18.8049\n",
      "Epoch 80/100\n",
      "1081/1081 [==============================] - 15s 14ms/step - loss: 20.5758 - val_loss: 18.9979\n",
      "Epoch 81/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.5152 - val_loss: 18.7267\n",
      "Epoch 82/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.4848 - val_loss: 18.8790\n",
      "Epoch 83/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.5180 - val_loss: 18.8008\n",
      "Epoch 84/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.4902 - val_loss: 18.7451\n",
      "Epoch 85/100\n",
      "1081/1081 [==============================] - 17s 15ms/step - loss: 20.5140 - val_loss: 18.8282\n",
      "Epoch 86/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 20.4655 - val_loss: 18.8325\n",
      "Epoch 87/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.4779 - val_loss: 19.1275\n",
      "Epoch 88/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.4572 - val_loss: 18.8055\n",
      "Epoch 89/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.4372 - val_loss: 18.7630\n",
      "Epoch 90/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.4305 - val_loss: 18.9536\n",
      "Epoch 91/100\n",
      "1081/1081 [==============================] - 18s 16ms/step - loss: 20.4907 - val_loss: 18.8876\n",
      "Epoch 92/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.4707 - val_loss: 18.8726\n",
      "Epoch 93/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 20.3754 - val_loss: 18.9479\n",
      "Epoch 94/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.4318 - val_loss: 19.2540\n",
      "Epoch 95/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.4252 - val_loss: 18.9219\n",
      "Epoch 96/100\n",
      "1081/1081 [==============================] - 17s 15ms/step - loss: 20.3633 - val_loss: 19.0759\n",
      "Epoch 97/100\n",
      "1081/1081 [==============================] - 17s 15ms/step - loss: 20.3253 - val_loss: 18.7896\n",
      "Epoch 98/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.4797 - val_loss: 19.3077\n",
      "Epoch 99/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 20.4010 - val_loss: 18.9444\n",
      "Epoch 100/100\n",
      "1081/1081 [==============================] - 18s 16ms/step - loss: 20.3013 - val_loss: 18.9194\n",
      "STACKLSTM learn complete\n",
      "Epoch 1/100\n",
      "1081/1081 [==============================] - 20s 17ms/step - loss: 133.9884 - val_loss: 62.2253\n",
      "Epoch 2/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 59.9069 - val_loss: 29.3697\n",
      "Epoch 3/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 33.7250 - val_loss: 22.3045\n",
      "Epoch 4/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 25.8459 - val_loss: 20.3656\n",
      "Epoch 5/100\n",
      "1081/1081 [==============================] - 17s 15ms/step - loss: 23.2721 - val_loss: 19.8384\n",
      "Epoch 6/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 22.6494 - val_loss: 19.6354\n",
      "Epoch 7/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 22.3001 - val_loss: 19.4104\n",
      "Epoch 8/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 22.0518 - val_loss: 19.5099\n",
      "Epoch 9/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.9006 - val_loss: 19.4193\n",
      "Epoch 10/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.8677 - val_loss: 19.4352\n",
      "Epoch 11/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 21.8024 - val_loss: 19.1661\n",
      "Epoch 12/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 21.7616 - val_loss: 19.1568\n",
      "Epoch 13/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 21.7359 - val_loss: 19.7550\n",
      "Epoch 14/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 21.7267 - val_loss: 19.1920\n",
      "Epoch 15/100\n",
      "1081/1081 [==============================] - 15s 14ms/step - loss: 21.6893 - val_loss: 19.1223\n",
      "Epoch 16/100\n",
      "1081/1081 [==============================] - 15s 14ms/step - loss: 21.7140 - val_loss: 19.5588\n",
      "Epoch 17/100\n",
      "1081/1081 [==============================] - 14s 13ms/step - loss: 21.6727 - val_loss: 19.6286\n",
      "Epoch 18/100\n",
      "1081/1081 [==============================] - 14s 13ms/step - loss: 21.6455 - val_loss: 19.1695\n",
      "Epoch 19/100\n",
      "1081/1081 [==============================] - 13s 12ms/step - loss: 21.6423 - val_loss: 19.0631\n",
      "Epoch 20/100\n",
      "1081/1081 [==============================] - 13s 12ms/step - loss: 21.6190 - val_loss: 19.5367\n",
      "Epoch 21/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 21.5984 - val_loss: 19.0268\n",
      "Epoch 22/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 21.5895 - val_loss: 19.3842\n",
      "Epoch 23/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 21.5837 - val_loss: 19.0949\n",
      "Epoch 24/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 21.5757 - val_loss: 19.1002\n",
      "Epoch 25/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.6266 - val_loss: 19.0441\n",
      "Epoch 26/100\n",
      "1081/1081 [==============================] - 18s 17ms/step - loss: 21.5989 - val_loss: 19.1017\n",
      "Epoch 27/100\n",
      "1081/1081 [==============================] - 18s 16ms/step - loss: 21.5433 - val_loss: 19.0352\n",
      "Epoch 28/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.5560 - val_loss: 19.0816\n",
      "Epoch 29/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 21.5673 - val_loss: 19.2242\n",
      "Epoch 30/100\n",
      "1081/1081 [==============================] - 15s 14ms/step - loss: 21.5009 - val_loss: 19.1519\n",
      "Epoch 31/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.5740 - val_loss: 19.3763\n",
      "Epoch 32/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 21.5519 - val_loss: 19.2085\n",
      "Epoch 33/100\n",
      "1081/1081 [==============================] - 17s 15ms/step - loss: 21.4991 - val_loss: 19.1311\n",
      "Epoch 34/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 21.5177 - val_loss: 19.2822\n",
      "Epoch 35/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 21.5279 - val_loss: 19.0127\n",
      "Epoch 36/100\n",
      "1081/1081 [==============================] - 17s 15ms/step - loss: 21.4712 - val_loss: 18.9962\n",
      "Epoch 37/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.5292 - val_loss: 19.3294\n",
      "Epoch 38/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.4799 - val_loss: 19.0797\n",
      "Epoch 39/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.5180 - val_loss: 19.1068\n",
      "Epoch 40/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.5552 - val_loss: 19.1997\n",
      "Epoch 41/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.4517 - val_loss: 19.0591\n",
      "Epoch 42/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.5247 - val_loss: 19.0501\n",
      "Epoch 43/100\n",
      "1081/1081 [==============================] - 17s 15ms/step - loss: 21.4498 - val_loss: 19.0995\n",
      "Epoch 44/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 21.5181 - val_loss: 19.0932\n",
      "Epoch 45/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 21.5369 - val_loss: 19.1066\n",
      "Epoch 46/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.4802 - val_loss: 19.2261\n",
      "Epoch 47/100\n",
      "1081/1081 [==============================] - 18s 16ms/step - loss: 21.4412 - val_loss: 19.1010\n",
      "Epoch 48/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 21.4247 - val_loss: 19.1045\n",
      "Epoch 49/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 21.4528 - val_loss: 19.2874\n",
      "Epoch 50/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.4819 - val_loss: 19.1254\n",
      "Epoch 51/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 21.4582 - val_loss: 19.0184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 21.4158 - val_loss: 19.1257\n",
      "Epoch 53/100\n",
      "1081/1081 [==============================] - 15s 14ms/step - loss: 21.4593 - val_loss: 19.1286\n",
      "Epoch 54/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.4452 - val_loss: 19.0280\n",
      "Epoch 55/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 21.4366 - val_loss: 19.2420\n",
      "Epoch 56/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.4513 - val_loss: 18.9696\n",
      "Epoch 57/100\n",
      "1081/1081 [==============================] - 17s 15ms/step - loss: 21.4202 - val_loss: 19.2441\n",
      "Epoch 58/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 21.3845 - val_loss: 19.0728\n",
      "Epoch 59/100\n",
      "1081/1081 [==============================] - 18s 16ms/step - loss: 21.3989 - val_loss: 19.1669\n",
      "Epoch 60/100\n",
      "1081/1081 [==============================] - 17s 15ms/step - loss: 21.4053 - val_loss: 18.9251\n",
      "Epoch 61/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 21.3882 - val_loss: 19.0491\n",
      "Epoch 62/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 21.4049 - val_loss: 19.1803\n",
      "Epoch 63/100\n",
      "1081/1081 [==============================] - 14s 13ms/step - loss: 21.4471 - val_loss: 19.1566\n",
      "Epoch 64/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 21.4145 - val_loss: 19.1652\n",
      "Epoch 65/100\n",
      "1081/1081 [==============================] - 17s 15ms/step - loss: 21.3538 - val_loss: 19.1773\n",
      "Epoch 66/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 21.3875 - val_loss: 19.0164\n",
      "Epoch 67/100\n",
      "1081/1081 [==============================] - 14s 13ms/step - loss: 21.3382 - val_loss: 19.0776\n",
      "Epoch 68/100\n",
      "1081/1081 [==============================] - 14s 13ms/step - loss: 21.4047 - val_loss: 18.9931\n",
      "Epoch 69/100\n",
      "1081/1081 [==============================] - 17s 15ms/step - loss: 21.3936 - val_loss: 19.0385\n",
      "Epoch 70/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 21.2996 - val_loss: 18.9840\n",
      "Epoch 71/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 21.3723 - val_loss: 19.0019\n",
      "Epoch 72/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.4221 - val_loss: 19.1310\n",
      "Epoch 73/100\n",
      "1081/1081 [==============================] - 15s 13ms/step - loss: 21.3902 - val_loss: 19.0357\n",
      "Epoch 74/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 21.3961 - val_loss: 18.9528\n",
      "Epoch 75/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.3793 - val_loss: 19.1155\n",
      "Epoch 76/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.3742 - val_loss: 19.1938\n",
      "Epoch 77/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.3343 - val_loss: 19.0947\n",
      "Epoch 78/100\n",
      "1081/1081 [==============================] - 18s 16ms/step - loss: 21.3163 - val_loss: 19.0977\n",
      "Epoch 79/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.3615 - val_loss: 18.9565\n",
      "Epoch 80/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 21.3328 - val_loss: 19.1555\n",
      "Epoch 81/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.3929 - val_loss: 19.1407\n",
      "Epoch 82/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.3393 - val_loss: 19.1134\n",
      "Epoch 83/100\n",
      "1081/1081 [==============================] - 18s 17ms/step - loss: 21.3353 - val_loss: 18.9747\n",
      "Epoch 84/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.3369 - val_loss: 19.2097\n",
      "Epoch 85/100\n",
      "1081/1081 [==============================] - 15s 14ms/step - loss: 21.4512 - val_loss: 19.0347\n",
      "Epoch 86/100\n",
      "1081/1081 [==============================] - 15s 14ms/step - loss: 21.3564 - val_loss: 19.0298\n",
      "Epoch 87/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 21.3758 - val_loss: 18.9768\n",
      "Epoch 88/100\n",
      "1081/1081 [==============================] - 15s 14ms/step - loss: 21.3189 - val_loss: 19.1507\n",
      "Epoch 89/100\n",
      "1081/1081 [==============================] - 14s 13ms/step - loss: 21.3470 - val_loss: 19.1829\n",
      "Epoch 90/100\n",
      "1081/1081 [==============================] - 15s 14ms/step - loss: 21.3576 - val_loss: 19.0765\n",
      "Epoch 91/100\n",
      "1081/1081 [==============================] - 15s 14ms/step - loss: 21.3168 - val_loss: 19.0622\n",
      "Epoch 92/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 21.3621 - val_loss: 19.2547\n",
      "Epoch 93/100\n",
      "1081/1081 [==============================] - 17s 15ms/step - loss: 21.3195 - val_loss: 18.9793\n",
      "Epoch 94/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 21.3265 - val_loss: 19.0126\n",
      "Epoch 95/100\n",
      "1081/1081 [==============================] - 16s 14ms/step - loss: 21.2794 - val_loss: 19.1030\n",
      "Epoch 96/100\n",
      "1081/1081 [==============================] - 14s 13ms/step - loss: 21.3116 - val_loss: 19.0798\n",
      "Epoch 97/100\n",
      "1081/1081 [==============================] - 16s 15ms/step - loss: 21.2573 - val_loss: 18.9603\n",
      "Epoch 98/100\n",
      "1081/1081 [==============================] - 16s 14ms/step - loss: 21.3379 - val_loss: 19.1070\n",
      "Epoch 99/100\n",
      "1081/1081 [==============================] - 15s 14ms/step - loss: 21.3128 - val_loss: 19.1616\n",
      "Epoch 100/100\n",
      "1081/1081 [==============================] - 17s 16ms/step - loss: 21.2735 - val_loss: 19.1588\n",
      "BILSTM learn complete\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'save_model_paath' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m ML\u001b[38;5;241m.\u001b[39mLSTM_learning()\n\u001b[0;32m     28\u001b[0m ML\u001b[38;5;241m.\u001b[39mSTACKLSTM_learning()\n\u001b[1;32m---> 29\u001b[0m \u001b[43mML\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBILSTM_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m ML\u001b[38;5;241m.\u001b[39mGRU_learning()\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m(ML)\n",
      "Cell \u001b[1;32mIn[9], line 177\u001b[0m, in \u001b[0;36mModel_learning.BILSTM_learning\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    169\u001b[0m     bilstm_model\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_univ,\n\u001b[0;32m    170\u001b[0m                      epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m    171\u001b[0m                      steps_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_x)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBATCH),\n\u001b[0;32m    172\u001b[0m                      validation_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_univ,\n\u001b[0;32m    173\u001b[0m                      validation_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_x)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBATCH))\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBILSTM learn complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 177\u001b[0m bilstm_model\u001b[38;5;241m.\u001b[39msave(\u001b[43msave_model_paath\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'save_model_paath' is not defined"
     ]
    }
   ],
   "source": [
    "directory = \"../../model/\"\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "directory = \"../../model/Fold\"\n",
    "for i in range(10):\n",
    "    if not os.path.exists(directory+str(i)+\"/\"):\n",
    "        os.makedirs(directory+str(i)+\"/\")\n",
    "\n",
    "for i in range(len(params)):\n",
    "    \n",
    "    ML = Model_learning(\n",
    "        FOLD = params.FOLD_NUM[i],\n",
    "        SEQ = params.SEQ_LEN[i],\n",
    "        BATCH = params.BATCH_SIZE[i],\n",
    "        HIDDEN = params.HIDDEN[i]\n",
    "    )\n",
    "    \n",
    "    ML.Print_params()\n",
    "    \n",
    "    ML.Add_data()\n",
    "    ML.Print_data()\n",
    "    ML.Stack_data()\n",
    "\n",
    "    ML.RNN_learning()\n",
    "    ML.LSTM_learning()\n",
    "    ML.STACKLSTM_learning()\n",
    "    ML.BILSTM_learning()\n",
    "    ML.GRU_learning()\n",
    "    \n",
    "    del(ML)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe71b12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d867a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
