{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "865e8764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import random as rn\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82d102e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(35)\n",
    "rn.seed(35)\n",
    "tf.random.set_seed(35)\n",
    "#print(device_lib.list_local_devices()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc5946a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../../data/BG Ex data/\"\n",
    "data = np.load(\"../../data/BG_Fold_Info.npz\", allow_pickle=True)\n",
    "\n",
    "train_files = data[\"train\"]\n",
    "test_files = data[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96eefda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "param12 = {\n",
    "    \"FOLD_NUM\": [i for i in range(10)],\n",
    "    \"SEQ_LEN\": [12],\n",
    "    \"BATCH_SIZE\": [128, 256],\n",
    "    \"HIDDEN\": [12, 24, 36]\n",
    "}\n",
    "\n",
    "param24 = {\n",
    "    \"FOLD_NUM\": [i for i in range(10)],\n",
    "    \"SEQ_LEN\": [24],\n",
    "    \"BATCH_SIZE\": [128, 256],\n",
    "    \"HIDDEN\": [24, 48, 72]\n",
    "}\n",
    "\n",
    "param36 = {\n",
    "    \"FOLD_NUM\": [i for i in range(10)],\n",
    "    \"SEQ_LEN\": [36],\n",
    "    \"BATCH_SIZE\": [128, 256],\n",
    "    \"HIDDEN\": [36, 72, 108]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c392f3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_grid(dict_):\n",
    "    return pd.DataFrame([row for row in product(*dict_.values())], columns=dict_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92cd79aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FOLD_NUM</th>\n",
       "      <th>SEQ_LEN</th>\n",
       "      <th>BATCH_SIZE</th>\n",
       "      <th>HIDDEN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>128</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>128</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>128</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>256</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>256</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>9</td>\n",
       "      <td>36</td>\n",
       "      <td>128</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>9</td>\n",
       "      <td>36</td>\n",
       "      <td>128</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>9</td>\n",
       "      <td>36</td>\n",
       "      <td>256</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>9</td>\n",
       "      <td>36</td>\n",
       "      <td>256</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>9</td>\n",
       "      <td>36</td>\n",
       "      <td>256</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     FOLD_NUM  SEQ_LEN  BATCH_SIZE  HIDDEN\n",
       "0           0       12         128      12\n",
       "1           0       12         128      24\n",
       "2           0       12         128      36\n",
       "3           0       12         256      12\n",
       "4           0       12         256      24\n",
       "..        ...      ...         ...     ...\n",
       "175         9       36         128      72\n",
       "176         9       36         128     108\n",
       "177         9       36         256      36\n",
       "178         9       36         256      72\n",
       "179         9       36         256     108\n",
       "\n",
       "[180 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = pd.concat([expand_grid(param12), expand_grid(param24), expand_grid(param36)])\n",
    "params.reset_index(inplace=True)\n",
    "params.drop(\"index\", axis=1, inplace=True)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41525637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9d9699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_learning:\n",
    "    \n",
    "    def __init__(self, FOLD, SEQ, BATCH, HIDDEN):\n",
    "        \n",
    "        self.train_x = None\n",
    "        self.train_y = None\n",
    "        \n",
    "        self.test_x = None\n",
    "        self.test_y = None\n",
    "        \n",
    "        self.train_univ = None\n",
    "        self.train_univ = None\n",
    "        \n",
    "        self.test_univ = None\n",
    "        self.test_univ = None\n",
    "        \n",
    "        self.FOLD=FOLD\n",
    "        self.SEQ=SEQ\n",
    "        self.BATCH=BATCH\n",
    "        self.HIDDEN=HIDDEN\n",
    "        \n",
    "    def __del__(self):\n",
    "        \n",
    "        print(\"Remove Success\\n\")\n",
    "        \n",
    "    def Print_params(self):\n",
    "        \n",
    "        print(\"FOLD:\", self.FOLD)\n",
    "        print(\"SEQ:\", self.SEQ)\n",
    "        print(\"BATCH:\", self.BATCH)\n",
    "        print(\"HIDDEN:\", self.HIDDEN)\n",
    "        \n",
    "    def Add_data(self):\n",
    "        \n",
    "        train_data = []\n",
    "        train_x, train_y = [], []\n",
    "        \n",
    "        for file in train_files[self.FOLD]:\n",
    "            train_data.append(np.load(file_path+file+\".npz\"))\n",
    "            \n",
    "        for i in range(len(train_data)):\n",
    "            train_x.append(train_data[i][\"gen\"+str(self.SEQ)])\n",
    "            train_y.append(train_data[i][\"y\"+str(self.SEQ)].astype(np.float32))\n",
    "            \n",
    "        test_data = []\n",
    "        test_x, test_y = [], []\n",
    "        \n",
    "        for file in test_files[self.FOLD]:\n",
    "            test_data.append(np.load(file_path+file+\".npz\"))\n",
    "            \n",
    "        for i in range(len(test_data)):\n",
    "            test_x.append(test_data[i][\"gen\"+str(self.SEQ)])\n",
    "            test_y.append(test_data[i][\"y\"+str(self.SEQ)].astype(np.float32))\n",
    "            \n",
    "        self.train_x = np.concatenate(train_x).reshape(-1, self.SEQ, 1)\n",
    "        self.train_y = np.concatenate(train_y).reshape(-1, 1)\n",
    "        \n",
    "        self.test_x = np.concatenate(test_x).reshape(-1, self.SEQ, 1)\n",
    "        self.test_y = np.concatenate(test_y).reshape(-1, 1)\n",
    "            \n",
    "    def Print_data(self):\n",
    "        \n",
    "        print(\"Train:\", self.train_x.shape, self.train_y.shape)\n",
    "        print(\"Test:\", self.test_x.shape, self.test_y.shape)\n",
    "        \n",
    "    def Stack_data(self):\n",
    "\n",
    "        train_univ = tf.data.Dataset.from_tensor_slices((self.train_x, self.train_y))\n",
    "        self.train_univ = train_univ.cache().shuffle(len(self.train_x)).batch(self.BATCH).repeat()\n",
    "\n",
    "        test_univ = tf.data.Dataset.from_tensor_slices((self.test_x, self.test_y))\n",
    "        self.test_univ = test_univ.batch(self.BATCH).repeat()\n",
    "        \n",
    "    def RNN_learning(self):\n",
    "        \n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        with tf.device('/CPU:0'):\n",
    "            rnn_model = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.SimpleRNN(self.HIDDEN, input_shape=self.train_x.shape[-2:]),\n",
    "                tf.keras.layers.Dense(1)\n",
    "            ])\n",
    "\n",
    "            rnn_model.compile(optimizer=\"adam\", loss=root_mean_squared_error)\n",
    "        \n",
    "        \n",
    "            rnn_model.fit(self.train_univ,\n",
    "                          epochs=100,\n",
    "                          steps_per_epoch=int(len(self.train_x)/self.BATCH),\n",
    "                          validation_data=self.test_univ,\n",
    "                          validation_steps=int(len(self.test_x)/self.BATCH))\n",
    "        \n",
    "        print(\"RNN learn complete\")\n",
    "        \n",
    "        rnn_model.save(\"../../model/Fold\"+str(self.FOLD)+\"/RNN_\"+str(self.SEQ)+\"_\"+str(self.HIDDEN)+\"_\"+str(self.BATCH)+\".h5\")\n",
    "\n",
    "    def LSTM_learning(self):\n",
    "        \n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        lstm_model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.LSTM(self.HIDDEN, input_shape=self.train_x.shape[-2:]),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "\n",
    "        lstm_model.compile(optimizer=\"adam\", loss=root_mean_squared_error)\n",
    "\n",
    "        lstm_model.fit(self.train_univ,\n",
    "                       epochs=100,\n",
    "                       steps_per_epoch=int(len(self.train_x)/self.BATCH),\n",
    "                       validation_data=self.test_univ,\n",
    "                       validation_steps=int(len(self.test_x)/self.BATCH))\n",
    "        \n",
    "        print(\"LSTM learn complete\")\n",
    "        \n",
    "        lstm_model.save(\"../../model/Fold\"+str(self.FOLD)+\"/LSTM_\"+str(self.SEQ)+\"_\"+str(self.HIDDEN)+\"_\"+str(self.BATCH)+\".h5\")     \n",
    "    \n",
    "    def STACKLSTM_learning(self):\n",
    "        \n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        stack_lstm_model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.LSTM(self.HIDDEN, input_shape=self.train_x.shape[-2:], return_sequences=True),\n",
    "            tf.keras.layers.LSTM(self.HIDDEN, return_sequences=False),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "\n",
    "        stack_lstm_model.compile(optimizer=\"adam\", loss=root_mean_squared_error)\n",
    "\n",
    "        stack_lstm_model.fit(self.train_univ,\n",
    "                             epochs=100,\n",
    "                             steps_per_epoch=int(len(self.train_x)/self.BATCH),\n",
    "                             validation_data=self.test_univ,\n",
    "                             validation_steps=int(len(self.test_x)/self.BATCH))\n",
    "        \n",
    "        print(\"STACKLSTM learn complete\")\n",
    "        \n",
    "        stack_lstm_model.save(\"../../model/Fold\"+str(self.FOLD)+\"/STACKLSTM_\"+str(self.SEQ)+\"_\"+str(self.HIDDEN)+\"_\"+str(self.BATCH)+\".h5\")     \n",
    "\n",
    "    def BILSTM_learning(self):\n",
    "        \n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        bilstm_model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Bidirectional(\n",
    "                tf.keras.layers.LSTM(self.HIDDEN),\n",
    "                input_shape=self.train_x.shape[-2:]\n",
    "            ),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "\n",
    "        bilstm_model.compile(optimizer=\"adam\", loss=root_mean_squared_error)\n",
    "\n",
    "        bilstm_model.fit(self.train_univ,\n",
    "                         epochs=100,\n",
    "                         steps_per_epoch=int(len(self.train_x)/self.BATCH),\n",
    "                         validation_data=self.test_univ,\n",
    "                         validation_steps=int(len(self.test_x)/self.BATCH))\n",
    "        \n",
    "        print(\"BILSTM learn complete\")\n",
    "        \n",
    "        bilstm_model.save(\"../../model/Fold\"+str(self.FOLD)+\"/BILSTM_\"+str(self.SEQ)+\"_\"+str(self.HIDDEN)+\"_\"+str(self.BATCH)+\".h5\")     \n",
    "\n",
    "    def GRU_learning(self):\n",
    "        \n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        gru_model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.GRU(self.HIDDEN, input_shape=self.train_x.shape[-2:]),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "\n",
    "        gru_model.compile(optimizer=\"adam\", loss=root_mean_squared_error)\n",
    "\n",
    "        gru_model.fit(self.train_univ,\n",
    "                      epochs=100,\n",
    "                      steps_per_epoch=int(len(self.train_x)/self.BATCH),\n",
    "                      validation_data=self.test_univ,\n",
    "                      validation_steps=int(len(self.test_x)/self.BATCH))\n",
    "        \n",
    "        print(\"GRU learn complete\")\n",
    "        \n",
    "        gru_model.save(\"../../model/Fold\"+str(self.FOLD)+\"/GRU_\"+str(self.SEQ)+\"_\"+str(self.HIDDEN)+\"_\"+str(self.BATCH)+\".h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e08510",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 0\n",
      "SEQ: 12\n",
      "BATCH: 128\n",
      "HIDDEN: 12\n",
      "Train: (136038, 12, 1) (136038, 1)\n",
      "Test: (15189, 12, 1) (15189, 1)\n",
      "Epoch 1/100\n",
      "1062/1062 [==============================] - 6s 5ms/step - loss: 190.9856 - val_loss: 185.0892\n",
      "Epoch 2/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 178.1552 - val_loss: 171.8102\n",
      "Epoch 3/100\n",
      "1062/1062 [==============================] - 5s 5ms/step - loss: 165.6498 - val_loss: 158.6735\n",
      "Epoch 4/100\n",
      "1062/1062 [==============================] - 5s 5ms/step - loss: 153.2971 - val_loss: 145.7059\n",
      "Epoch 5/100\n",
      "1062/1062 [==============================] - 5s 5ms/step - loss: 141.2941 - val_loss: 132.9681\n",
      "Epoch 6/100\n",
      "1062/1062 [==============================] - 5s 4ms/step - loss: 129.6711 - val_loss: 120.5511\n",
      "Epoch 7/100\n",
      "1062/1062 [==============================] - 5s 4ms/step - loss: 118.5009 - val_loss: 108.6284\n",
      "Epoch 8/100\n",
      "1062/1062 [==============================] - 5s 5ms/step - loss: 107.9438 - val_loss: 97.2486\n",
      "Epoch 9/100\n",
      "1062/1062 [==============================] - 5s 4ms/step - loss: 97.9156 - val_loss: 86.5103\n",
      "Epoch 10/100\n",
      "1062/1062 [==============================] - 5s 5ms/step - loss: 88.6062 - val_loss: 76.6023\n",
      "Epoch 11/100\n",
      "1062/1062 [==============================] - 5s 4ms/step - loss: 80.0572 - val_loss: 67.4508\n",
      "Epoch 12/100\n",
      "1062/1062 [==============================] - 5s 4ms/step - loss: 71.9618 - val_loss: 59.3352\n",
      "Epoch 13/100\n",
      "1062/1062 [==============================] - 5s 4ms/step - loss: 64.6730 - val_loss: 52.3937\n",
      "Epoch 14/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 58.0456 - val_loss: 46.1162\n",
      "Epoch 15/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 52.0537 - val_loss: 41.4417\n",
      "Epoch 16/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 46.7788 - val_loss: 36.5769\n",
      "Epoch 17/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 42.1453 - val_loss: 32.9941\n",
      "Epoch 18/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 38.2125 - val_loss: 30.3542\n",
      "Epoch 19/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 34.8292 - val_loss: 28.3078\n",
      "Epoch 20/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 32.1172 - val_loss: 27.4034\n",
      "Epoch 21/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 29.9260 - val_loss: 25.6901\n",
      "Epoch 22/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 28.2494 - val_loss: 24.9837\n",
      "Epoch 23/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 27.0215 - val_loss: 24.0598\n",
      "Epoch 24/100\n",
      "1062/1062 [==============================] - 3s 3ms/step - loss: 26.1047 - val_loss: 23.3200\n",
      "Epoch 25/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 25.4413 - val_loss: 22.7671\n",
      "Epoch 26/100\n",
      "1062/1062 [==============================] - 3s 3ms/step - loss: 24.8574 - val_loss: 22.3882\n",
      "Epoch 27/100\n",
      "1062/1062 [==============================] - 5s 4ms/step - loss: 24.4958 - val_loss: 21.8786\n",
      "Epoch 28/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 24.0703 - val_loss: 21.6514\n",
      "Epoch 29/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 23.7979 - val_loss: 21.3052\n",
      "Epoch 30/100\n",
      "1062/1062 [==============================] - 5s 4ms/step - loss: 23.5501 - val_loss: 21.1587\n",
      "Epoch 31/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 23.3606 - val_loss: 20.8781\n",
      "Epoch 32/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 23.1939 - val_loss: 20.6050\n",
      "Epoch 33/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 23.0119 - val_loss: 20.7105\n",
      "Epoch 34/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.9323 - val_loss: 20.5529\n",
      "Epoch 35/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.8577 - val_loss: 20.3162\n",
      "Epoch 36/100\n",
      "1062/1062 [==============================] - 5s 5ms/step - loss: 22.7968 - val_loss: 20.2475\n",
      "Epoch 37/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.6425 - val_loss: 20.0290\n",
      "Epoch 38/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.6151 - val_loss: 19.9012\n",
      "Epoch 39/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.6189 - val_loss: 19.9686\n",
      "Epoch 40/100\n",
      "1062/1062 [==============================] - 4s 3ms/step - loss: 22.4455 - val_loss: 19.7040\n",
      "Epoch 41/100\n",
      "1062/1062 [==============================] - 5s 4ms/step - loss: 22.5342 - val_loss: 19.6954\n",
      "Epoch 42/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.4439 - val_loss: 19.9056\n",
      "Epoch 43/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.4634 - val_loss: 19.8899\n",
      "Epoch 44/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.3740 - val_loss: 19.7327\n",
      "Epoch 45/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.3742 - val_loss: 19.4585\n",
      "Epoch 46/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.3845 - val_loss: 19.5419\n",
      "Epoch 47/100\n",
      "1062/1062 [==============================] - 4s 3ms/step - loss: 22.3550 - val_loss: 19.5250\n",
      "Epoch 48/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.2996 - val_loss: 19.3768\n",
      "Epoch 49/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.2480 - val_loss: 19.5561\n",
      "Epoch 50/100\n",
      "1062/1062 [==============================] - 5s 5ms/step - loss: 22.3325 - val_loss: 19.3682\n",
      "Epoch 51/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.2609 - val_loss: 19.3725\n",
      "Epoch 52/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.2561 - val_loss: 19.4247\n",
      "Epoch 53/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.2559 - val_loss: 19.3469\n",
      "Epoch 54/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.2794 - val_loss: 20.6396\n",
      "Epoch 55/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.2449 - val_loss: 19.2815\n",
      "Epoch 56/100\n",
      "1062/1062 [==============================] - 5s 5ms/step - loss: 22.2007 - val_loss: 19.2191\n",
      "Epoch 57/100\n",
      "1062/1062 [==============================] - 5s 4ms/step - loss: 22.1916 - val_loss: 19.7357\n",
      "Epoch 58/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.2590 - val_loss: 19.9379\n",
      "Epoch 59/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.2055 - val_loss: 19.8307\n",
      "Epoch 60/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.2270 - val_loss: 19.1446\n",
      "Epoch 61/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.2247 - val_loss: 19.1204\n",
      "Epoch 62/100\n",
      "1062/1062 [==============================] - 5s 5ms/step - loss: 22.1627 - val_loss: 20.0945\n",
      "Epoch 63/100\n",
      "1062/1062 [==============================] - 5s 5ms/step - loss: 22.1539 - val_loss: 19.8222\n",
      "Epoch 64/100\n",
      "1062/1062 [==============================] - 5s 5ms/step - loss: 22.2019 - val_loss: 19.0619\n",
      "Epoch 65/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.1938 - val_loss: 19.2751\n",
      "Epoch 66/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.1469 - val_loss: 19.2421\n",
      "Epoch 67/100\n",
      "1062/1062 [==============================] - 5s 4ms/step - loss: 22.1216 - val_loss: 19.3537\n",
      "Epoch 68/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.2093 - val_loss: 19.4365\n",
      "Epoch 69/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.2087 - val_loss: 19.3675\n",
      "Epoch 70/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.1247 - val_loss: 19.9567\n",
      "Epoch 71/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.1138 - val_loss: 19.1764\n",
      "Epoch 72/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.1999 - val_loss: 19.4237\n",
      "Epoch 73/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.1452 - val_loss: 19.0725\n",
      "Epoch 74/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.2158 - val_loss: 20.1996\n",
      "Epoch 75/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.1232 - val_loss: 19.5630\n",
      "Epoch 76/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.1531 - val_loss: 19.1458\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.1358 - val_loss: 18.9332\n",
      "Epoch 78/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.1219 - val_loss: 19.4385\n",
      "Epoch 79/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.1751 - val_loss: 19.5390\n",
      "Epoch 80/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.0828 - val_loss: 18.9284\n",
      "Epoch 81/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.1695 - val_loss: 19.1967\n",
      "Epoch 82/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.1266 - val_loss: 19.1108\n",
      "Epoch 83/100\n",
      "1062/1062 [==============================] - 5s 4ms/step - loss: 22.1592 - val_loss: 19.0682\n",
      "Epoch 84/100\n",
      "1062/1062 [==============================] - 5s 4ms/step - loss: 22.1332 - val_loss: 18.9939\n",
      "Epoch 85/100\n",
      "1062/1062 [==============================] - 5s 4ms/step - loss: 22.0759 - val_loss: 19.0782\n",
      "Epoch 86/100\n",
      "1062/1062 [==============================] - 5s 5ms/step - loss: 22.1321 - val_loss: 19.2844\n",
      "Epoch 87/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.1440 - val_loss: 19.1302\n",
      "Epoch 88/100\n",
      "1062/1062 [==============================] - 5s 4ms/step - loss: 22.1200 - val_loss: 18.9549\n",
      "Epoch 89/100\n",
      "1062/1062 [==============================] - 5s 5ms/step - loss: 22.0811 - val_loss: 18.9266\n",
      "Epoch 90/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.1413 - val_loss: 19.1955\n",
      "Epoch 91/100\n",
      "1062/1062 [==============================] - 3s 3ms/step - loss: 22.0540 - val_loss: 19.2289\n",
      "Epoch 92/100\n",
      "1062/1062 [==============================] - 3s 2ms/step - loss: 22.1662 - val_loss: 18.9317\n",
      "Epoch 93/100\n",
      "1062/1062 [==============================] - 4s 3ms/step - loss: 22.0586 - val_loss: 18.9436\n",
      "Epoch 94/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.1191 - val_loss: 19.1797\n",
      "Epoch 95/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.0873 - val_loss: 19.1217\n",
      "Epoch 96/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.1066 - val_loss: 19.0004\n",
      "Epoch 97/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.1192 - val_loss: 19.1131\n",
      "Epoch 98/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.0717 - val_loss: 19.0092\n",
      "Epoch 99/100\n",
      "1062/1062 [==============================] - 4s 4ms/step - loss: 22.0851 - val_loss: 19.1076\n",
      "Epoch 100/100\n",
      "1061/1062 [============================>.] - ETA: 0s - loss: 22.1374"
     ]
    }
   ],
   "source": [
    "directory = \"../../model/\"\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "directory = \"../../model/Fold\"\n",
    "for i in range(10):\n",
    "    if not os.path.exists(directory+str(i)+\"/\"):\n",
    "        os.makedirs(directory+str(i)+\"/\")\n",
    "\n",
    "for i in range(len(params)):\n",
    "    \n",
    "    ML = Model_learning(\n",
    "        FOLD = params.FOLD_NUM[i],\n",
    "        SEQ = params.SEQ_LEN[i],\n",
    "        BATCH = params.BATCH_SIZE[i],\n",
    "        HIDDEN = params.HIDDEN[i]\n",
    "    )\n",
    "    \n",
    "    ML.Print_params()\n",
    "    \n",
    "    ML.Add_data()\n",
    "    ML.Print_data()\n",
    "    ML.Stack_data()\n",
    "    \n",
    "    ML.RNN_learning()\n",
    "    ML.LSTM_learning()\n",
    "    ML.STACKLSTM_learning()\n",
    "    ML.BILSTM_learning()\n",
    "    ML.GRU_learning()\n",
    "    \n",
    "    del(ML)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe71b12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d867a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
